<!-- Starting most of the nodes needed for grasping unknown object demo-->
<!-- December 2011 -->

<!-- To start in addition -->

<!-- next few comment lines are old -->
<!-- activate driver/can: /home/grasp/Dropbox/TUW/AmtecArm/usefullArmStuff/activatecandevice.py => is now done in system files (insmod of can driver, make nodes pcan0/pcan1-->
<!-- start node for arm control: roslaunch cob_arm solo.launch -->
<!-- start node for arm dashboard: roslaunch cob_dashboard dashboard.launch -->
<!-- initialize Arm: click button init at dashboard user interface -->
<!-- if robot should mirror values from simulationserver (controlled by OR) start: roslaunch mechanismst_to_goaltraj mechanismst_to_goaltraj.launch  -->
<!-- for steps for arm controlling there is now a GUI: rospackage tuw_robot_control, program: tuw_robot_control_GUI.py -->
<!-- at the end, start OR-programm: rosrun tt_planning sceneupdaterTUWDemo.py-->


<launch>

<!-- start ni-driver, publish TF frames for camera1 and TF connection between moving camera1 and camera1  -->
  <include file="$(find graspingDemoTUWUObjects)/launch/openni_nodeTUW.launch"/>						<!-- cam1: driver, var. TF -->

<!-- start ni-driver, publish TF frames for camera2 and TF connection between arm-checkerboard; moving camera2 and camera2  -->
  <!--include file="$(find graspingDemoTUWUObjects)/launch/openni_nodeTUWKinect2.launch"/-->						<!-- cam2: driver, var. TF -->


<!-- start checkerboard/publish_TF_frames_cam1_only for cam1: TF frames for world, checkerboard and arm_base-->
  <include file="$(find checkerboard_detection)/launch/publish_TF_frames_cam1_only.launch"/>					<!-- cam1: basic tf -> needed!  -->


<!-- start checkerboard_detection/cameraposedetectionKinect1.launch for cam1 -->
  <!--include file="$(find checkerboard_detection)/launch/cameraposedetectionKinect1.launch"/-->				<!-- cam1: posedetection -->


<!-- start ar_kinect/ar_kinect.launch for cam1 (replaced checkerboard_detection)-->
  <include file="$(find ar_kinect)/launch/ar_kinect.launch"/>				<!-- cam1: cameraposedetection -->



<!-- start checkerboard_detection/camera pose detection for cam2 -->
  <!--include file="$(find checkerboard_detection)/launch/cameraposedetectionKinect2.launch"/-->				<!-- cam2: posedetection -->


<!-- start steady camera_tf publisher for camera1-->
  <include file="$(find camera_tf_steady_pub)/launch/camera_tf_steady_pub.launch"/>						<!-- cam1: tf_steady_pub -->

<!-- start steady camera_tf publisher for camera2-->
  <!--include file="$(find camera_tf_steady_pub)/launch/camera_tf_steady_pubKinect2.launch"/-->					<!-- cam2: tf_steady_pub -->


<!-- start rviz for visualization -->
  <include file="$(find graspingDemoTUWUObjects)/launch/rviz_demo.launch"/>						<!-- rviz -->

<!-- start Trigger for single shots for both cameras -->
  <include file="$(find Trigger)/launch/startup.launch"/>									<!-- trigger -->

<!-- start SimultionServer fro package openrave_robot_control (jsk-ros-pkg/stacks/openrave_planning) -->
  <include file="$(find graspingDemoTUWUObjects)/launch/SimulationServer.launch"/>						<!-- SimulationServer -->

<!-- start pc_merge (merging pointclouds of 2 cameras, cutting scene, filtering, sampling) -->
<!--!!!!and!!!!!!!!! start kates preprocessing: publishes XYZRC taking depth/points coming from merge (does again cutting) !!!!!!!!!! -->
  <include file="$(find toolsdf)/launch/pc_merge_1cam_nobox.launch"/>									<!-- pc_merge -->

<!-- start pc_to_iv (saves the incomming point cloud as inventor file filname.iv) -->
  <include file="$(find pc_to_iv)/launch/pc_to_iv.launch"/>									<!-- pc_to_iv -->

<!-- start manage_grasphypothesis (receives grasp hypothesis, orders them and returns best one if service is called) -->
  <include file="$(find manage_grasphypothesis)/launch/manage_grasphypothesis.launch"/>						<!-- manage_grasphypothesis -->


<!-- start detection of grasping points using SVM  -->
  <!--include file="$(find calc_grasppoints_svm)/launch/calc_grasppoints_svm.launch"/-->					<!-- calc_grasppoints -->

<!-- starts OpenRAVE scene with roboter and so on -->
  <!--include file="$(find graspingDemoTUWUObjects)/launch/sceneupdaterTUWDemo.launch"/-->					<!-- scene -->

<!-- if cameras are detected, cameraposenodes can be stopped by: rosnode kill /checkerboard_pose (and /checkerboard_pose2) -->
<!-- now the demo has to be started by going to graspingDemoTUWUObjects/src and starting it by: ./sceneupdaterTUWDemo.py-->
<!-- after that a shot can be triggerd by following command:                                                            -->
<!-- rostopic pub -1 /SS/doSingleShot std_msgs/String "makeitso"                                                        -->

</launch>

